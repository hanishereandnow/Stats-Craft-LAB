# 7. 컨볼루션

두 확률변수 $X$, $Y$의 합 $X+Y$는 또다른 확률변수가 됩니다.
$X+Y$의 분포를 $X$와 $Y$의 분포로 표현할 때, 컨볼루션의 개념이 수반됩니다.

## 7.1. 연속확률변수의 컨볼루션

### 7.1.1. 연속확률변수의 합

두 연속확률변수 $X$, $Y$에 대하여 새로운 확률변수 $S=X+Y$의 PDF는

$$
f_S(s)=\int_{-\infty}^\infty f_{XY}(s-u,u)\,du
\tag{$\ast$}
$$

로 주어집니다.
당연히, 위의 식에서 $u$는 dummy variable입니다.
그러니까, 위의 식에서 $u$말고 다른 어떤 변수로 넣어도, 상관없습니다.
만약, $X$와 $Y$가 독립이면

$$
f_S(s)=\int_{-\infty}^\infty f_X(s-u)f_Y(u)\,du
\tag{$\ast\ast$}
$$

와 같이 쓸 수도 있습니다.
$(\ast\ast)$에 대한 증명을 강의에서는 다음과 같이 하고 있습니다.

$$
\begin{align*}
F_S(s)
&=P(S\le s)\\
&=\iint_{x+y\le s}f_{XY}(x,y)\,dx\,dy
\end{align*}
$$

위의 계산에서
$y$는 $-\infty$부터 $\infty$까지 움직이고, $y$가 하나의 특정한 값으로 고정되었을 때, $x$는 $-\infty$부터 $s-y$까지 움직이므로

$$
\begin{align*}
F_S(s)
&=\int_{-\infty}^\infty\int_{-\infty}^{s-y}f_{XY}(x,y)\,dx\,dy\\
&=\int_{-\infty}^\infty\int_{-\infty}^{s-y}f_X(x)f_Y(y)\,dx\,dy\\
&=\int_{-\infty}^\infty f_Y(y)\int_{-\infty}^{s-y}f_X(x)\,dx\,dy\\
&=\int_{-\infty}^\infty f_Y(y)F_X(s-y)\,dy
\end{align*}
$$

양변을 $s$에 대해 미분하면

$$
\begin{align*}
f_S(s)
&=\frac d{ds}\int_{-\infty}^\infty f_Y(y)F_X(s-y)\,dy\\
&\stackrel\star=\int_{-\infty}^\infty\frac d{ds}f_Y(y)F_X(s-y)\,dy\\
&=\int_{-\infty}^\infty f_Y(y)f_X(s-y)\,dy
\end{align*}
$$

이 된다는 것입니다.
따라서 $(\ast\ast)$가 성립합니다.

$$
\begin{align*}
F_S(s)
&=\int_{-\infty}^s\int_{-\infty}^\infty f_{XY}(t-y,y)\,dy\,dt\\
\end{align*}
$$

양변을 미분하면

$$f_S(s)=\int_{-\infty}^\infty f_{XY}(s-y,y)\,dy$$

이고, 따라서 $(\ast)$가 성립하며, $(\ast\ast)$도 성립합니다.

#### 정의 : convolution (continuous)
일반적으로, 두 함수 $f:\mathbb R\to\mathbb R$, $g:\mathbb R\to\mathbb R$에 대하여 새로운 함수 $f\ast g:\mathbb R\to\mathbb R$을
$$(f\ast g)(s)=\int_{-\infty}^\infty f(u)g(s-u)\,du$$
로 정의할 때, $f\ast g$는 $f$와 $g$의 convolution이라고 불립니다.

따라서, 앞서의 결과를 정리하면, 두 연속확률변수 $X$와 $Y$에 대하여 $S=X+Y$의 확률밀도함수 $f_S$는 $f_X$와 $f_Y$의 convolution입니다.
다시 말해,

$$f_S=f_X\ast f_Y$$

라고 쓸 수 있고, $f_S(s)$는

$$f_S(s)=\int_{-\infty}^\infty f(u)g(s-u)\,du$$

를 만족시키는 함수입니다.

강의에는 포함되어 있지 않지만, convolution 연산 $\ast$의 기본적인 성질들에 대해 더 적어보려 합니다.
$\ast$은 교환법칙과 결합법칙이 성립합니다.
먼저, $\ast$가 교환법칙이 성립한다는 것은, $f\ast g=g\ast f$가 성립한다는 것입니다.
왜냐하면

$$
\begin{align*}
(f\ast g)(s)
&=\int_{-\infty}^\infty f(t)g(s-t)\,dt\\
&=-\int^{-\infty}_\infty f(s-u)g(u)\,du\\
&=\int_{-\infty}^\infty g(u)f(s-u)\,du\\
&=(g\ast f)(s)
\end{align*}
$$

이기 때문입니다.
또한, 결합법칙도 성립합니다.
왜냐하면 (적분기호의 위끝과 아래끝을 생략했습니다.)

$$
\begin{align*}
((f\ast g)\ast h)(s)
&=(h\ast(f\ast g))(s)\\
&=\int_{-\infty}^\infty (f\ast g)(s-t)h(t)\,dt\\
&=\int_{-\infty}^\infty\int_{-\infty}^\infty f(u)g(s-t-u)\,du\,h(t)\,dt\\
&=\int_{-\infty}^\infty f(u)\int_{-\infty}^\infty h(t)g(s-u-t)\,dt\,du\\
&=\int_{-\infty}^\infty f(u)(h\ast g)(s-u)\,du\\
&=\int_{-\infty}^\infty f(u)(g\ast h)(s-u)\,du\\
&=(f\ast(g\ast h))(s)
\end{align*}
$$

이기 때문입니다.
따라서, 실수 전체에서 적분가능한 함수들의 집합을 $\mathcal F$이라고 하면, $(\mathcal F,\ast)$는 abelian group입니다.

다시 강의로 돌아오겠습니다.
강의에서는 convolution에 대한 개념정리를 다음과 같이 하고 있습니다(표현을 조금 바꾸어서 적었습니다.).

$x(t)\ast h(t)=y(t)$

1. Select functions $x(t)$ and $h(t)$, transform $h(t)$ symmetrically about $t=0$ to get $x(t)$ and $h(-t)$.
2. Shift the symmetrically transformed function $h(-t)$ by $s$ to get $x(t)$ and $h(s-t)$.
3. Product the overlapped region of $y=x(t)$ and $y=h(s-t)$ by calculating

$$\int_{-\infty}^\infty x(t)h(s-t)\,ds$$

#### 예시

convolution에 대한 예시로서, 두 함수 $x(u)$와 $h(u)$가

$$
\begin{align*}
x(u)&=
\begin{cases}
1&(0\le u\le 1)\\
0&(\text{otherwise})
\end{cases}\\[10pt]
h(u)&=
\begin{cases}
\frac12&(0\le u\le2)\\
0&(\text{otherwise})
\end{cases}
\end{align*}
$$

와 같이 주어지는 경우입니다.
$x(u)$가 0이 아닌 $u$들의 집합을 $\text{supp}(x)$라고 하고, $h(t-u)$가 0이 아닌 집합을 $\text{supp}(h)$라고 하면, 

$$
\begin{align*}
\text{supp}(x)
&=\{u:0\le u\le1\}\\
&=[0,1]\\
\text{supp}(h)
&=\{u:0\le t-u\le 2\}\\
&=\{u:t-2\le u\le t\}\\
&=[t-2,t]
\end{align*}
$$

이고, 따라서

$$
\begin{matrix}
            &\text{supp}(x)\cap\text{supp}(h)&x(t)h(t-u)&(x\ast h)(t)\\[10pt]
t\le 0      &\varnothing                     &          &0           \\[10pt]
0\le t\le1  &[0,t]                           &\frac12   &\frac12t    \\[10pt]
1\le t\le2  &[1,2]                           &\frac12   &\frac12     \\[10pt]
2\le t\le3  &[t-2,1]                         &\frac12   &\frac12(3-t)\\[10pt]
t\ge0       &\varnothing                     &          &0           \\
\end{matrix}
$$

입니다.
다시 말해, 함수 $y=\left(x\ast h\right)(t)$는

$$
\left(x\ast h\right)(t)=
\begin{cases}
0                   &(t\le0)\\
\frac12t            &(0\le t\le1)\\
\frac12             &(1\le t\le2)\\
-\frac12t+\frac32   &(2\le t\le3)\\
0                   &(t\ge3)
\end{cases}
$$

![]({{site.url}}\images\2023-03-26-kocw_stats\stats_15-4.png){: .img-50-center}

와 같이 됩니다.

#### 예시 : Erlang distribution

convolution을 사용하면 이전에 미뤄두었던 Erlang distribution에 대한 PDF 식을 유도할 수 있습니다.
다시 복습해보면, 사건의 평균 발생주기가 $\lambda$인 Poisson process에서 사건이 시각 $T_0$, $T_1$, $T_2$에 일어났다고 가정하면, $T_1-T_0$과 $T_2-T_1$는 exponential distribution을 따릅니다.
$X=T_1-T_0$, $Y=T_2-T_1$라고 두면,

$$
\begin{align*}
f_X(x)&=
\begin{cases}
\lambda e^{-\lambda x}  &x\ge0\\
0                       &(\text{otherwise})
\end{cases}\\[20pt]
f_Y(y)&=
\begin{cases}
\lambda e^{-\lambda y}  &y\ge0\\
0                       &(\text{otherwise})
\end{cases}
\end{align*}
$$

입니다.
이때, $T_3-T_1$는 Erlang-$2$ distribution을 따른다고 할 수 있습니다.
확률변수 $S$를 $S=T_3-T_1=X+Y$라고 두면
Erlang-$k$ distribution의 식

$$
f_X(x)=\frac{\lambda^kx^{k-1}}{(k-1)!}e^{-\lambda x}\qquad x\ge0
\tag{$\ast\ast\ast$}
$$

에 $k=2$를 대입한

$$
f_S(s)=\lambda^2se^{-\lambda s}\qquad s\ge0
$$

와 같은 식이 유도되어야 합니다.
실제로 convolution을 통해 식을 유도해보기 전에, 두 함수 $f_X(u)$와 $f_Y(s-u)$가 0이 되지 않는 구간 (두 함수 $f_X(u)$와 $f_Y(s-u)$의 support)을 계산해보면

$$
\begin{align*}
\text{supp}(f_x(u))
&=\{u:f_X(u)\gt0\}\\
&=\{u:u\gt0\}\\
\text{supp}(f_Y(s-u))
&=\{u:f_Y(s-u)\gt0\}\\
&=\{u:s-u\gt0\}\\
&=\{u:u\lt s\}
\end{align*}
$$

입니다.
(일반적으로 함수 $f$의 support는 $f(x)\ne0$인 $x$들의 집합에 closure를 취한 것이지만, 여기에서는 closure를 취하지 않은 버전의 support를 구했습니다.)
따라서, 두 함수의 곱 $f_X(u)f_Y(s-u)$은 $s\gt0$일 때, 0이 아닌 구간 $0\lt u\lt s$을 가지지만, 그렇지 않은 경우에는 모든 실수 $u$에 대하여 $f_X(u)f_Y(s-u)=0$입니다.
그러므로,

$$
\begin{aligned}
(f_X\ast f_Y)(s)
&=\int_{-\infty}^\infty f_X(u)f_Y(s-u)\,du\\
&=\int_0^sf_X(u)f_Y(s-u)\,du\\
&=\int_0^s\lambda e^{-\lambda u}\times\lambda e^{-\lambda(s-u)}\,du\\
&=\lambda^2\int_0^se^{-\lambda s}\,du\\
&=\lambda^2se^{-\lambda s}
\end{aligned}\qquad(s\ge0)
$$

이 되어, Erlang-$2$ PDF의 식과 일치합니다.

일반적인 Erlang-$k$ distribution의 PDF 식을 유도하기 위해 다시, 사건의 평균 발생주기가 $\lambda$인 Poisson process에서 사건이 시각 $T_1$, $T_2$, $\cdots$, $T_k$에 일어났다고 가정하겠습니다.
그리고, $X_i=X_i-X_{i-1}$로 두면 ($i=1,2,\cdots,k$)

$$
f_{X_i}(x_i)=
\begin{cases}
\lambda e^{-\lambda x_i}  &x_i\ge0\\
0                         &(\text{otherwise})
\end{cases}
$$

입니다.
아까, convolution 연산 $\ast$가 결합법칙을 만족한다고 했으므로, 새로운 확률변수 $S=X_1+\cdots+X_k$에 대하여

$$f_S=f_{X_1}\ast f_{X_2}\ast\cdots\ast f_{X_k}$$

는 잘 정의된 함수입니다.

$k=1$, $k=2$일 때에는 이미 $(\ast\ast\ast)$이 성립함을 확인했습니다.
$k-1$의 경우에 $(\ast\ast\ast)$이 성립한다고 가정하고, $k$인 경우를 보겠습니다.
$X=T_1-T_0$, $Y=T_k-T_1$이라고 하면, 이미

$$
\begin{align*}
f_X(x)&=
\begin{cases}
\lambda e^{-\lambda x}  &x\ge0\\
0                       &(\text{otherwise})
\end{cases}\\[20pt]
f_Y(y)&=
\begin{cases}
\frac{\lambda^{k-1}y^{k-2}}{(k-2)!}e^{-\lambda y}  &y\ge0\\
0                                                  &(\text{otherwise})
\end{cases}
\end{align*}
$$

를 알고 있는 셈입니다.
이때, $S=T_k-T_0=X+Y$로 나타낼 수 있는 새로운 확률변수 $S$가 $(\ast\ast\ast)$를 만족시키는지 확인하려고 합니다.
convolution으로 $f_S$를 계산해보면, $s\ge0$일 때

$$
\begin{align*}
f_S(s)
&=\left(f_{X_1}\ast \left(f_{X_2}\ast\cdots\ast f_{X_k}\right)\right)(s)\\
&=(f_X\ast f_Y)(s)\\
&=(f_X\ast f_Y)(s)\\
&=\int_{-\infty}^\infty\lambda e^{-\lambda u}\times\frac{\lambda^{k-1}(s-u)^{k-2}}{(k-2)!}e^{-\lambda(s-u)}\,du\\
&=\int_0^s\lambda e^{-\lambda u}\times\frac{\lambda^{k-1}(s-u)^{k-2}}{(k-2)!}e^{-\lambda(s-u)}\,du\\
&=\frac{\lambda^k}{(k-2)!}\int_0^se^{-\lambda s}(s-u)^{k-2}\,du\\
&=\frac{\lambda^ke^{-\lambda s}}{(k-2)!}\int_s^0v^{k-2}\times(-1)\,dv\\
&=\frac{\lambda^ke^{-\lambda s}}{(k-2)!}\int_0^sv^{k-2}\,dv\\
&=\frac{\lambda^ke^{-\lambda s}}{(k-2)!}\times\frac{s^{k-1}}{k-1}\\
&=\frac{\lambda^ks^{k-1}}{(k-1)!}e^{-\lambda s}
\end{align*}
$$

이 되어 정말로 $(\ast\ast\ast)$가 성립합니다.
따라서, Erlang-$k$ distribution의 PDF 식이 증명되었습니다.

### 7.1.2. moments of sum of random variables

두 확률변수 $X$, $Y$의 합 $S=X+Y$의 평균은 각 확률변수의 합과 같습니다 ;

$$E[X+Y]=E[X]+E[Y]$$

연속확률변수의 경우에만 증명해보면

$$
\begin{align*}
E[X+Y]
&=\iint_{\mathbb R^2}(x+y)f_{XY}(x,y)\,dx\,dy\\
&=\iint_{\mathbb R^2}xf_{XY}(x,y)\,dx\,dy+\iint_{\mathbb R^2}yf_{XY}(x,y)\,dx\,dy\\
&=\int_{\mathbb R}x\int_{\mathbb R}f_{XY}(x,y)\,dy\,dx
+\int_{\mathbb R}y\int_{\mathbb R}f_{XY}(x,y)\,dx\,dy\\
&=\int_{\mathbb R}xf_X(x)\,dx+\int_{\mathbb R}yf_Y(y)\,dy\\
&=E[X]+E[Y]
\end{align*}
$$

가 됩니다.
이산확률변수의 경우에도 비슷하게 증명할 수 있습니다.

$S=X+Y$의 분산은

$$
\begin{align*}
{\sigma_S}^2
&=E\left[\left\{(X+Y)-(E[X]+E[Y])\right\}^2\right]\\
&=E\left[\left\{(X+Y)-(\mu_X+\mu_Y)\right\}^2\right]\\
&=E\left[\left\{(X-\mu_X)+(Y-\mu_Y)\right\}^2\right]\\
&=E\left[(X-\mu_X)^2+2(X-\mu_X)(Y-\mu_Y)+(Y-\mu_Y)^2\right]\\
&=E\left[(X-\mu_X)^2\right]
+2E\left[(X-\mu_X)(Y-\mu_Y)\right]
+E\left[(Y-\mu_Y)^2\right]\\
&={\sigma_X}^2+2\text{cov}(X,Y)+{\sigma_Y}^2
\end{align*}
$$

로 계산될 수 있습니다.
만약, $X$와 $Y$가 uncorrelated이면,

$${\sigma_S}^2={\sigma_X}^2+{\sigma_Y}^2$$

이 성립하는 것도 쉽게 확인할 수 있습니다.

마찬가지로, $S=X_1+\cdots+X_n$일 때,

$$
\begin{align*}
E[S]&=E[X_1]+\cdots+E[X_n]\\
{\sigma_S}^2&={\sigma_{X_1}}^2+\cdots+{\sigma_{X_n}}^2+2\sum_{i\ne j}\text{cov}(X_i,X_j)
\end{align*}
$$

가 될 것입니다.

## 7.2. 이산확률변수의 컨볼루션

### 7.2.1. 이산확률변수의 합

지금까지 연속확률변수의 convolution에 대해 이야기되었습니다.
즉, 이산확률변수 $X$와 $Y$에 대하여 새로운 확률변수 $S=X+Y$의 확률밀도함수는

$$f_S=f_X\ast f_Y$$

의 관계를 만족시키며, 구체적으로는

$$(f_X\ast f_Y)(s)=\int_{-\infty}^\infty f(s-u)g(u)\,du$$

와 같이 나타난다고 했습니다.

이번에는 이산확률변수 $X$와 $Y$에 대한 convolution을 다룹니다.
$X$가 이산확률변수라고 하는 것은, $X$가 취할 수 있는 값이 유한하거나, countable하다는 뜻이라고 했습니다.
그런데, convolution의 계산을 편하게 하기 위해서, $X$가 정수의 값만 가질 수 있다고 가정하고 있습니다.
마찬가지로 $Y$도 그 값이 정수인 경우만 이야기하고 있습니다.

이 경우에 convolution의 정의나 $S=X+Y$의 확률함수 식 계산은 연속확률분포에서보다 훨씬 간단합니다.
연속확률변수의 경우에서는 누적분포함수 $F_S(s)$를 먼저 구하기 위해서, 좌표평면 위에 $x+y\le s$라고 하는 부등식의 영역을 고려한 뒤 적분했어야 했습니다.
하지만, 이산확률변수의 경우, 확률질량함수 $P_X(s)$를 바로 구해도 됩니다.
그러면 부등식의 영역 대신

$$x+y=s,\qquad x,y\in\mathbb Z$$

와 같은 간단한 방정식을 생각하게 됩니다.
이때, $\mathbb Z$는 정수들의 집합으로, $x,y\in Z$는 $x$와 $y$가 모두 정수라는 사실을 표현한 것입니다.
이 디오판토스 방정식의 해는 무한히 많지만 countable한 정도로 많고, 그 해를 모두 쉽게 표현할 수 있습니다 ;

$$
\begin{cases}
x=k\\
y=s-k
\end{cases}\quad(k\in\mathbb Z)
$$

즉

$$
\{(x,y)\in\mathbb Z^2:x+y=s\}=\bigcup_{k=-\infty}^\infty\{(k,s-k)\}
$$

입니다.
따라서,

$$
\begin{align*}
P_S(s)
&=P(S=s)\\
&=P(X+Y=s)\\
&=\sum_{k=-\infty}^\infty P(X=k, Y=s-k)
\end{align*}
$$

만약, $X$와 $Y$가 독립이면

$$
P_S(s)
=\sum_{k=-\infty}^\infty P_X(k)P_Y(s-k)
$$

이것은 연속확률변수에서의 식과 거의 유사합니다.
integral이었던 것이 summation으로 바뀌었고, PDF였던 것이 PMF로 바뀌었습니다.

#### 정의 : convolution (discrete)
일반적으로, 두 함수 $f:\mathbb Z\to\mathbb R$, $g:\mathbb Z\to\mathbb R$에 대하여 새로운 함수 $f\ast g:\mathbb Z\to\mathbb R$을
$$(f\ast g)(s)=\sum_{k=-\infty}^\infty f(k)g(s-k)$$
로 정의할 때, $f\ast g$는 $f$와 $g$의 convolution이라고 불립니다.

dicrete 버전의 convolution도 마찬가지로 결합법칙과 교환법칙이 성립합니다 ;

$$
\begin{align*}
((f\ast g)\ast h)(s)
&=\sum_{k\in\mathbb Z}(f\ast g)(k)h(s-k)\\
&=\sum_{k\in\mathbb Z}\sum_{m\in\mathbb Z}f(m)g(k-m)h(s-k)\\
&=\sum_{m\in\mathbb Z}f(m)\sum_{k\in\mathbb Z}g(k-m)h(s-k)\\
&=\sum_{m\in\mathbb Z}f(m)\sum_{j\in\mathbb Z}g(j)h(s-m-j)\\
&=\sum_{m\in\mathbb Z}f(m)(g\ast h)(s-m)\\
&=(f\ast(g\ast h))(s)\\[10pt]
(f\ast g)(s)
&=\sum_{k\in\mathbb Z}f(k)g(s-k)\\
&=\sum_{j\in\mathbb Z}f(s-j)g(j)\\
&=\sum_{j\in\mathbb Z}g(j)f(s-j)\\
&=(g\ast f)(s)
\end{align*}
$$

#### 예시

동전을 하나 던져서 앞면이 나오면 $X=1$, 뒷면이 나오면 $X=0$으로 대응시키는 확률변수 $X$를 생각하면, $X$의 PMF는

$$
\begin{cases}
P_X(0)&=\frac12\\
P_X(1)&=\frac12
\end{cases}
$$

입니다.
또, 주사위를 하나 던져서 나온 숫자를 $Y$라고 하면, $Y$의 PMF는

$$
\begin{cases}
P_X(1)&=\frac16\\
P_X(2)&=\frac16\\
P_X(3)&=\frac16\\
P_X(4)&=\frac16\\
P_X(5)&=\frac16\\
P_X(6)&=\frac16
\end{cases}
$$

입니다.
그러면, 당연히 $X$와 $Y$는 독립입니다.
새로운 확률변수 $S$를 $S=X+Y$로 정의하면 $S$의 값은 $1$, $2$, $\cdots$, $7$의 값을 가질 수 있고

$$
\begin{cases}
P_S(1)&=\frac1{12}\\
P_S(2)&=\frac16\\
P_S(3)&=\frac16\\
P_S(4)&=\frac16\\
P_S(5)&=\frac16\\
P_S(6)&=\frac16\\
P_S(7)&=\frac1{12}
\end{cases}
$$

와 같이 계산됩니다.
이것은 

$$P_S(s)=(P_X\ast P_Y)(s)=\sum_{k=-\infty}^\infty P_X(k)P_Y(s-k)$$

로 계산될 수 있습니다.

### 7.2.2. 이항분포와 컨볼루션

강의의 마지막에는 이항분포를 따르는 두 확률변수의 합에 대해 이야기합니다.
만약

$$X\sim B(n,p),\quad Y\sim B(m,p)$$

이면,

$$
\begin{align*}
P_X(x)&=\binom nxp^x(1-p)^{n-x}&&x=0,1,\cdots,n\\
P_Y(y)&=\binom myp^y(1-p)^{m-y}&&y=0,1,\cdots,m
\end{align*}
$$

입니다.
두 확률변수의 합인 $S=X+Y$의 확률질량함수를 구하기 위해서는 위의 두 PMF를 convolve하면 되는데 그때 사용되는 두 값은 $P_X(k)$와 $P_Y(s-k)$입니다.
만약 $k\lt0$이면 $P_X(k)=0$이고, $k\gt s$이면 $P_Y(s-k)=0$입니다.
따라서, convolution을 계산할 때 $0\le k\le s$인 항들만 계산하면 됩니다;

$$
\begin{align*}
P_S(s)
&=(P_X\ast P_Y)(s)\\
&=\sum_{k=-\infty}^\infty P_X(k)P_Y(s-k)\\
&=\sum_{k=0}^sP_X(k)P_Y(s-k)\\
&=\sum_{k=0}^s\binom nkp^k(1-p)^{n-k}\times\binom m{s-k}p^{s-k}(1-p)^{m-s+k}\\
&=\sum_{k=0}^s\binom nk\binom m{s-k}p^s(1-p)^{m+n-s}\\
&=\left(\sum_{k=0}^s\binom nk\binom m{s-k}\right)p^s(1-p)^{m+n-s}\\
&\stackrel\ast=\binom{m+n}sp^s(1-p)^{m+n-s}\\
\end{align*}
$$

입니다.
이때 $\star$는 Vandermonde's identity라는 이름을 가지고 있다고 합니다.
강의에서는 이 식에 대한 증명을 조합론적으로 하고 있습니다.
만약 $m+n$개의 대상 중 $s$개를 뽑는다고 할 때, 특정한 $m$개의 대상을 A집단, 나머지를 B집단이라고 하면, 그 경우의 수는 A에서 $k$개를 뽑고 B에서 나머지 $s-k$개를 뽑는 경우를 다 더한 값과 같습니다.
따라서 Vandermonde's inequality가 성립합니다.

이 등식에 대한 조금 더 수학적인 증명도 찾아보려 했는데, [위키피디아](https://en.wikipedia.org/wiki/Vandermonde's_identity)에서는 이항정리을 이용한 증명과 rectangular grid의 길찾기 문제를 이용한 증명이 나와있습니다.
하지만 근본적으로는 다 비슷한 증명이 아닐까 하는 생각이 듭니다.
[이곳](https://math.stackexchange.com/q/219938)에는 수학적 귀납법을 사용한 증명도 있습니다.

### 7.2.3. 푸아상 분포와 컨볼루션

Poisson distribution에 대한 합도 생각해볼 수 있습니다.
만약

$$X\sim\text{Pois}(\lambda),\quad Y\sim\text{Pois}(\nu)$$

이면,

$$
\begin{align*}
P_X(x)&=\frac{\lambda^x}{x!}e^{-\lambda}\qquad(x=0,1,2,\cdots)\\
P_Y(y)&=\frac{\nu^y}{y!}e^{-\nu}\qquad(y=0,1,2,\cdots)\\
\end{align*}
$$

입니다.
이번에도, $k\lt0$이거나 $k\gt s$이면 두 값 $P_X(k)$와 $P_Y(s-k)$ 중 하나의 값이 $0$이 되고, 따라서 $0\le k\le s$인 항들만 계산하면 됩니다;

$$
\begin{align*}
P_S(s)
&=(P_X\ast P_Y)(s)\\
&=\sum_{k=-\infty}^\infty P_X(k)P_Y(s-k)\\
&=\sum_{k=0}^sP_X(k)P_Y(s-k)\\
&=\sum_{k=0}^s\frac{\lambda^k}{k!}e^{-\lambda}\times\frac{\nu^{s-k}}{(s-k)!}e^{-\nu}\\
&=\frac{e^{-(\lambda+\nu)}}{s!}\sum_{k=0}^s\frac{s!}{k!(s-k)!}\lambda^k\nu^{s-k}\\
&=\frac{e^{-(\lambda+\nu)}}{s!}(\lambda+\nu)^s\\
&=\frac{(\lambda+\nu)^s}{s!}e^{-(\lambda+\nu)}
\end{align*}
$$

따라서, $S$는 parameter가 $\lambda+\mu$인 Poisson distribution을 따릅니다 ;

$$X+Y\sim\text{Pois}(\lambda+\nu)$$

이것은, 어떻게 생각하면 당연한 결과입니다.
상점의 예를 들면, A 상점에는 시간당 $\lambda$명의 사람이 들어오고 B 상점에는 시간당 $\nu$명의 사람이 들어올 때, 어느 시점의 한 시간동안 A상점에 들어온 손님의 수를 $X$, B상점에 들어온 손님의 수를 $Y$라고 하면, $X$와 $Y$는 각각 위의 확률질량함수 $P_X$와 $P_Y$를 갖는다고 했었습니다.
만약, $S$를 $S=X+Y$라고 하면, (두 상점에 모두 들어간 손님이 없다고 할 때) $S$는 상점 A 혹은 상점 B에 들어간 손님의 수라고 볼 수 있고, 시간당 $\lambda+\nu$명의 손님이 들어온다고 예상되므로, $S\sim\text{Pois}(\lambda+\nu)$인 것입니다.

다시 말해, $X$는 어떤 사건 $A$가 단위시간당 $\lambda$번 일어난다고 가정할 때의 어느 단위시간동안 사건이 일어난 수이고, $Y$는 어떤 사건 $B$가 단위시간당 $\nu$번 일어난다고 가정할 때의 어느 단위시간동안 사건이 일어난 수일 때, 단위시간당 $\lambda+\nu$번의 사건 A 혹은 사건 B가 일어나므로, $X+Y$는 $\text{Pois}(\lambda+\nu)$를 따릅니다.

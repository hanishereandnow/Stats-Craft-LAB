# 파이썬통계학교과서> 4. 통계모델 기본

# 4.1. 통계모델

## 4.1.1.  모델

- 여기서 얘기하는 모델은 프라모델의 모델, 즉 모형이라고 할 수 있다.

## 4.1.2. 모델링

- 모델은 만드는 것을 모델링이라고 한다. 통계모델을 만드는 것은 통계모델링이다.

## 4.1.3. 모델은 무엇에 도움이 되나

- 비행기의 형태를 한 작은 모형을 만들면 진짜 비행기를 사용하지 않고도 진짜 비행기의 특성, 예를 들어 그 비행기가 날 수 있는지, 바람이 불면 어떻게 흔들리는지 등을 알아볼 수 있다.
- 실제 세계의 모형(모델)을 이용하는 것으로 현실 세계의 이해와 예측에 활용할 수 있다.

## 4.1.4. 복잡한 세계를 단순화하다

- 맥주 매상 데이터 모델화를 시도해본다고 가정하다.
- 맥주 매상에 영향을 미치는 요인은 무수히 많을 수도 있다. 하지만, 모든 요인을 다 고려하는 것은 비효율적이다. 모든 요인을 계산에 집어넣으면 인간은 이해할 수 없는 수수께끼 같은 인과율이 나올 수 있다.
- 따라서, 여러 가지 요소들을 과감히 무시하고, 맥주 매상은 기온이 올라가면 늘어난다는 측면만 주목하는 쪽이 간단하다.
- 반대로 더우면 맥주를 마시고 싶어진다고 하면 이해하기는 쉽지만, 너무 단순하게만 하면 현실과 맞지 않는 모델이 될 수도 있다.
- 즉, 인간이 이해할 수 있을 만큼 단순하고 그러면서도 복잡한 현실상을 어느 정도 잘 설명할 수 있는 복잡한 세계를 위한 단순한 모델은 구축해야 한다.

## 4.1.5. 복잡한 현상을 특정한 관점에서 다시 보게 한다.

- 모델은 실제 현상을 어떤 측면에서 바라본 결과라고 얘기할 수도 있다.
- 기온과 맥주 매상관계라는 ‘그날의 기온’이라는 관점에서 바라본 모델을 구축해볼 수 있다.
- 한편, 맥주를 자주 마시는 인구수와 맥주 매상이라는 소비자수 추이에 따른 관점에서 바라본 모델을 구축 할 수도 있다.
- 어느 쪽이 올바르다고 하는 것이 아니라, 분석의 목적에 맞춰서 작성하는 모델과 주목하는 관점을 바꾸는 것이 가능하다.

## 4.1.6. 수리모델

- 수리모델은 현상을 수식으로 표현한 모델이다.
- 예를 들어 다음 수식으로 맥주 매상이 결정된다고 하면 맥주와 기온의 관계가 보다 명확해진다.
    - 맥주 매상(만원) = $20+4 \times 기온(℃)$

## 4.1.7. 확률모델

- 수리모델 중에서도 특히 확률적인 표현이 있는 모델을 확률모델이라고 한다.
- 예를 들어 정규분포를 가정했을 때의 맥주 매상을 기온으로 설명하는 확률모델은 다음과 같이 표현할 수 있다.
    - 맥주매상 $\sim N(20 + 4 \times 기온, \sigma^2)$
    - 맥주매상 $= 20 + 4 \times 기온 + \epsilon$,    $\epsilon \sim N(0, \sigma^2)$

## 4.1.8. 통계모델

- 통계모델이란 데이터에 적합하게 구축된 확률모델을 말한다.
- 확률모델과 통계모델의 구별은 그렇게 엄밀하지 않아서 같은 의미로 사용되는 경우도 있다.

## 4.1.9. 확률분포와 통계모델

- 통계모델을 사용하면 확률분포의 모수(파라미터)의 변화 패턴을 명확히 할 수 있다.
- 예를 들어 ‘기온이 올라가면 맥주 매상의 평균값이 증가한다’와 같이 모델 구조를 명확히 할 수있다.

# 4.2. 통계모델을 만드는 방법

## 4.2.1. 종속변수와 독립변수

- 종속변수(반응변수)란 어떤 요인에 종속된 변수, 다시 말해 어떤 변화에 응답하는 변수이다.
    - 앞선 모델에서는 맥주 매상이 종속 변수이다.
- 독립변수(설명변수)는 대상의 변화를 설명하는 변수이며, 모델 내의 다른 대상에 영향을 받지 않는 독립적인 변수이다.
    - 앞선 모델에서는 기온, 날시, 맥주 가격 이 독립변수이다.
- 즉, 독립변수를 사용해서 종속변수를 모델링한다.

## 4.2.2. 파라메트릭 모델

- 가능한 한 현상을 단순화해서 소수의 파라미터만 사용하는 모델을 파라메트릭 모델이라고 한다.
- 소수의 파라미터만으로 모델을 추정하여 식의 형태가 단순하고 해석이 용이하다.

## 4.2.3. 논파라메트릭 모델

- 소수가 아닌 다양한 파라미터를 사용한다.
- 표현은 쉽게 할 수 있지만 복잡한 모델이 될 수 있어 추정과 해석이 어려워 질 수 있다.

## 4.2.4. 선형모델

- 선형모델은 종속변수와 독립변수의 관계를 선형으로 보는 모델이다.

## 4.2.5. 계수와 가중치

- 통계모델에 사용되는 파라미터를 계수라고 한다.
- 통계학에서는 ‘계수’라고 부르지만 머신러닝에서는 같은 내용을 ‘가중치’라고 표현한다.

## 4.2.6. 검정을 이용한 변수 선택

- 종속변수가 맥주 매상이고 기온이 독립변수인 모델을 가정해보자.
- 기온이 맥주 매상에 얼마나 영향을 미치는 지는 계수 (또는 가중치)로 표현할 수 있다.
- 통계적가설검정을 이용하는 경우 다음과 같은 가설을 세운다.
    - 귀무가설: 독립변수의 계수는 0이다.
    - 대립가설: 독립변수의 계수는 0이 아니다.
- 귀무가설이 기각되는 경우에는 기온에 대한 계수가 0이 아니라고 판단할 수 있기 때문에 모델에 기온이라는 독립변수가 필요하다고 판단할 수 있다.
- 반대고 귀무가설을 기각할 수 없을 때는 모델은 간단한 쪽이 좋다는 원칙에 의해 기온 변수를 모델에서 제거한다.
    - 이 경우 기온이라는 독립변수가 모델에서 유일한 경우 Null 모델(독립변수가 없는 모델)이 된다.
- 이 외에도 분산분석이라는 검정 방법 등도 있다.

## 4.2.7.  정보 기준을 이용한 변수 선택

- 모델 선택의 또 다른 방법은 ‘정보 기준’을 사용하는 것이다.
- 정보 기준이란 추정한 모델의 좋은 정도를 정량화한 지표이다.
    - 주로 아케이케 정보 기준(AIC) 등이 사용
    - AIC 가 작을수록 좋은 모델이라고 판단한다.
    - 모델에서 가능한 변수의 패턴을 망라하여 모델을 구축하고, 각 모델의 AIC 를 비교한다.
    - AIC 가 가장 작은 모델을 채택함으로써 변수 선택을 실행한다.

## 4.2.8. 모델 평가

- 추정한 모델을 무조건 신뢰하는 것은 위험하다. 추정한 모델을 평가해야 한다.
- 평가에는 여러가지 평가 관점이 있다. 그 중 대표적인 평가 관점이 예측 정확도 이다.(오차값 확인)

# 4.3. 데이터의 표현과 모델의 명칭

## 4.3.1. 정규선형모델

- 종속변수가 정규분포를 따르는 것을 가정한 선형모델을 정규선형모델이라고 한다.
- 종속변수가 정규분포를 따른다고 가정하기 때문에 종속변수는 $- \infin \sim +\infin$  의 범위를 갖는 연속형 변수가 된다.

## 4.3.2. 회귀분석

- 정규선형모델 중 독립변수가 연속형 변수인 모델을 회귀분석이라고 한다. 회귀모델 이라고도 한다.

## 4.3.3. 다중회귀분석

- 회귀분석 중에서도 독립변수가 여러 개 있는 것을 다중회귀분석이라고 한다.
- 독립변수가 1개인 회귀분석을 단일회귀분석이라고도 한다.

# 4.4. 파라미터 추정: 우도의 최대화

## 4.4.1. 우도

- 파라미터가 정해져 있을 때 표본을 얻을 수 있는 확률(밀도)을 우도라고 한다. 우도는 다른말로 가능도라고 하며 Likelihood 를 뜻하는 그럴듯한 정도를 말한다.
- 우도(가능도, Likelihood) : 고정된 관측값이 어떠한 확률분포에서 어느 정도의 확률로 나타나는지에 대한 확률
    
    ![Untitled](%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%8A%E1%85%A5%E1%86%AB%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%80%E1%85%AD%E1%84%80%E1%85%AA%E1%84%89%E1%85%A5%204%20%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20%E1%84%80%E1%85%B5%E1%84%87%E1%85%A9%E1%86%AB%20b495ec01c1e54d07b4270f85bf5a7185/Untitled.png)
    
    [https://youtu.be/XepXtl9YKwc](https://youtu.be/XepXtl9YKwc)
    

## 4.4.2. 로그우도

- 우도에 로그를 취한 것을 로그우도라고 한다. 로그를 취하면 계산이 편해지는 경우가 많다.
    - 로그 성질에 의한 로그우도의 장점
        - 단조증가: 우도를 최대로 하는 파라미터를 찾은 결과는 로그우도를 최대로 하는 파라미터를 찾은 결과와 일치한다.
        - 곱셈이 덧셈으로 바뀐다.
        - 절댓값이 극단적으로 작은 값이 되기 어렵다.

## 4.4.3. 최대우도법

- 최대우도법은 우도나 로그우도의 결과를 최대로 하기 위한 파라미터를 추정할 때 사용하는 방법

[https://angeloyeo.github.io/2020/07/17/MLE.html](https://angeloyeo.github.io/2020/07/17/MLE.html)

## 4.4.4. 장애모수

- 직접적인 관심의 대상이 아닌 파라미터를 장애모수라고 한다.
- 정규분포의 모수는 평균과 분산 2가지이다. 하지만, 평균값으로 분산을 계산할 수 있다.
즉, 평균을 추정할 수 있다면 분산 또한 덩달아서 알 수 있다.
- 따라서, 분산이라는 파라미터에 관심을 두지 않고, 이미 알고 있는 것으로 취급하는데 정규분포를 추정할 때의 최대우도법에서는 분산 $\sigma^2$ 을 장애모수로 취급한다.

# 4.5. 파라미터 추정: 손실의 최소화

## 4.5.1. 손실함수

- 손실함수는 파라미터 추정을 할 때 손실을 최소화하는 목적으로 사용된다.
- 이때 손실을 어떻게 정의하느냐가 문제가 된다.
    - 이에 따라 최적의 모델을 추정할 수 있다.

## 4.5.2. 잔차

- 실제 종속변수의 값과 모델을 이용해 계산한 종속변수의 추정치와의 차이를 잔차(residuals)라고 한다.
    - $residuals = y - \hat{y}$

## 4.5.3. 잔차제곱합

- 잔차의 합을 그대로 손실 지표로 사용할 수 없다.
    - 잔차들간의 합은 0이 되어 정확한 오차를 파악할 수 없기 때문이다.
- 잔차를 제곱해서 합계를 구한 것을 잔차제곱합이라고 한다.
    - 잔체제곱합 $= \displaystyle\sum_{i=1}^{N}[(y_{\tiny i} -\hat{y}_{\tiny i})^2]$

## 4.5.4. 최소제곱법(OLS: Ordinary Least Squared)

- 잔차제곱합을 최소로 하는 파라티를 채용하는 방법을 최소제곱법이라고 한다.
- 손실함수로 잔차제곱합을 사용하여 손실을 최소로 하는 파라미터를 추정치로 하는 방법이라고도 할 수 있다.

## 4.5.5. 최소제곱법과 최대우도법의 관계

- 최소제곱법을 이용한 파라미터 추정치는 모집단분포가 정규분포임을 가정했을 때의 최대우도법과 일치한다.

## 4.5.6. 오차함수

- 머신러닝 분야에서 로그우도의 부호를 바꾼 것을 ‘오차함수’ 라고 부른다.
- 최소제곱법은 모집단부포가 정규분포임을 가정했을 때의 오차함수를 최소화하는 것이라고 해석할 수 있다.

# 4.6. 예측 정확도의 평가와 변수 선택

## 4.6.1. 적합도와 예측 정확도

- 적합도: 가지고 있는 데이터에 대해 모델을 적용했을 때 들어맞는 정도 (X_train, y_train)
- 예측 정확도: 아직 얻지 못한 데이터에 대해 모델을 적용했을 때 들어맞는 정도 (X_trst, y_test)

## 4.6.2. 과적합(오버피팅)

- 적합도는 높은데, 예측 정확도가 낮아지는 경우 과적합.
- 과적합은 가지고 있는 데이터에 지나치게 복잡한 모델을 만들면 발생.

## 4.6.3. 변수 선택의 의의

- 과적합을 하게 되는 흔한 원인으로 독립변수를 너무 많이 늘리는 경우가 있다.
- 필요 없는 독립변수를 제외하는 것만으로도 예측 정확도가 높아질 가능성이 있다.
    - 그러나 필요 없는 독립변수를 추가해도 적합도가 높아진다는 것이 알려져 있다.
- 따라서, 변수 선택으로 필요없는 독립변수를 제거 할 필요가 있다.

## 4.6.4. 교차검증

- 교차검증(크로스 밸리데이션; Cross Validation)
    - 데이터를 일정한 규칙에 따라 훈련 데이터와 테스트 데이터로 나누어 테스트 데이터에 대한 예측 정확도를 평가하는 방법
    - 교차검증은 크게 ‘리브-p-아웃 교차검증(leave-p-out CV)과 ‘K-fold CV’ 두 종류로 나눌 수 있다.
        - 리브-p-아웃 교차검증 :
        가지고 있는 데이터에서 p개의 데이터를 추출하고 남은 데이터를 테스트 데이터로 사용하는 방법
            - 예를 들어 리브-2-아웃 교차검증은 가지고 있는 데이터에서 2개를 추출하여 훈련 데이터로 이용한다. 그리고 나머지 데이터로 예측 정확도를 평가한다.
            - 데이터를 p개 추출하는 방법에는 여러가지 조합이 있을 수 있으므로 그것들을 시험하여 정확도의 평균값을 평가값으로 사용한다.
        - k-fold CV :
        가지고 있는 데이터를 K개의 그룹으로 나눈다. 그 그룹 중 하나를 추출하여 테스트 데이터로 사용한다. 이것을 K번 반복하여 예측 정확도의 평균값을 평가값으로 사용한다.
            - 샘플사이즈가 100인 경우 리브-1-아웃 교차검증과 100겹 교차검증은 둘다 1개의 데이터를 추출하여 테스트로 삼기 때문에 의미가 같다.

## 4.6.5. 아케이준 정보 기준

- AIC가 작을수록 좋은 모델이라고 간주한다.
    - $AIC = -2 \times (최대로그우도 - 추정된 파라미터 수)$
    - 로그우도가 크면 클수록 적합도가 높다고 볼 수 있다. 하지만 적합도를 높게 하는 데만 주력하면 아직 얻지 못한 데이터에 대한 예측 오차가 커진다.
    때문에, AIC로 추정한 파라미터 수를 적합도에 대한 페널티로 사용한다.
    즉, 독립변수가 많아지면 로그우도가 커진다.

## 4.6.6. 상대 엔트로피

- 상대 엔트로피는 2개의 확률밀도함수의 로그 차를 상대 엔트로피의 기댓값으로 간주하고,  이 값을 이용해 확률분포의 차이를 측정하는 지표이다.

## 4.6.7. AIC와 교차검증

- AIC 최소 기준에서 변수를 선택하면 모르는 데이터에 대한 예측 정확도를 높일 수 있는 가능성이 있다.
